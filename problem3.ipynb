{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from keras.datasets import mnist \n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "# the data, shuffled and split between train and test sets \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "input_dim = 784 #28*28 \n",
    "X_train = X_train.reshape(60000, input_dim) \n",
    "X_test = X_test.reshape(10000, input_dim) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255\n",
    "output_dim = nb_classes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[10,:])\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    x -= np.max(x)\n",
    "    sm = (np.exp(x).T / np.sum(np.exp(x),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "def ReLu(x, derivative=False):\n",
    "    if(derivative==False):\n",
    "        return x*(x > 0)\n",
    "    else:\n",
    "        return 1*(x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateWeights():\n",
    "    ##Initialization of the Weights and the Biases with the random gaussian function with mean zeron, and variance between 1/sqtr(num_inputs_layer)\n",
    "    \n",
    "    ninputs = 784\n",
    "    wl1 = 500 ##Number of neurons in the first layer\n",
    "    wl2 = 300 ##Number of neurons in the second layer\n",
    "    nclass = 10 ##Numer of the class, in this case it is the number of the digits.\n",
    "    \n",
    "    #layer1\n",
    "    w1 = np.random.normal(0, ninputs**-0.5, [ninputs,wl1])\n",
    "    b1 = np.random.normal(0, ninputs**-0.5, [1,wl1])\n",
    "    \n",
    "    #Layer2\n",
    "    w2 = np.random.normal(0, wl1**-0.5, [wl1,wl2])\n",
    "    b2 = np.random.normal(0, wl1**-0.5, [1,wl2])\n",
    "\n",
    "    #Layer3\n",
    "    w3 = np.random.normal(0, wl2**-0.5, [wl2,nclass])\n",
    "    b3 = np.random.normal(0, wl2**-0.5, [1,nclass])\n",
    "    \n",
    "    return [w1,w2,w3,b1,b2,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dropout(x, dropout_percent):\n",
    "    mask = np.random.binomial([np.ones_like(x)],(1-dropout_percent))[0]  / (1-dropout_percent)\n",
    "    return x*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights, x, dropout_percent=0):\n",
    "    \n",
    "    w1,w2,w3,b1,b2,b3  = weights \n",
    "    \n",
    "    #1-Hidden Layer\n",
    "    first = ReLu(x@w1+b1)\n",
    "    first = Dropout(first, dropout_percent)\n",
    "\n",
    "    #2-Hidden Layer\n",
    "    second = ReLu(first@w2+b2)\n",
    "    second = Dropout(second, dropout_percent)\n",
    "    \n",
    "    #Output Layer\n",
    "    return [first, second, Softmax(second@w3+b3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, y):\n",
    "    hit = 0\n",
    "    output = np.argmax(output, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    for y in zip(output, y):\n",
    "        if(y[0]==y[1]):\n",
    "            hit += 1\n",
    "\n",
    "    p = (hit*100)/output.shape[0]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    if(x!=0):\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "def log(y):\n",
    "    return [[log2(nx) for nx in x]for x in y]\n",
    "\n",
    "def cost(Y_predict, Y_right, weights, nabla):\n",
    "    w1,w2,w3,b1,b2,b3  = weights\n",
    "    weights_sum_square = np.mean(w1**2) + np.mean(w2**2) + np.mean(w3**2)\n",
    "    Loss = -np.mean(Y_right*log(Y_predict) + (1-Y_right)*log(1-Y_predict)) + nabla/2 *  weights_sum_square\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(weights, x, t, outputs, eta, gamma, nabla, cache=None):\n",
    "    \n",
    "    w1,w2,w3,b1,b2,b3  = weights\n",
    "    \n",
    "    \n",
    "    if(cache==None):\n",
    "            vw1 = np.zeros_like(w1)\n",
    "            vw2 = np.zeros_like(w2)\n",
    "            vw3 = np.zeros_like(w3)\n",
    "            vb1 = np.zeros_like(b1)\n",
    "            vb2 = np.zeros_like(b2)\n",
    "            vb3 = np.zeros_like(b3)\n",
    "    else:\n",
    "        vw1,vw2,vw3,vb1,vb2,vb3 = cache\n",
    "    \n",
    "    first, second, y = outputs\n",
    "   \n",
    "    w3_delta = (t-y)\n",
    "   \n",
    "    w2_error = w3_delta@w3.T\n",
    "    \n",
    "    w2_delta = w2_error * ReLu(second,derivative=True)\n",
    "\n",
    "    w1_error = w2_delta@w2.T\n",
    "    w1_delta = w1_error * ReLu(first,derivative=True)\n",
    "    \n",
    "    eta = -eta/x.shape[0]\n",
    " \n",
    "    vw3 = gamma*vw3 + eta * (second.T@w3_delta + nabla*w3)\n",
    "    vb3 = gamma*vb3 + eta * w3_delta.sum(axis=0)\n",
    "\n",
    "    vw2 = gamma*vw2 + eta * (first.T@w2_delta + nabla*w2)\n",
    "    vb2 = gamma*vb2 + eta * w2_delta.sum(axis=0)\n",
    "\n",
    "    vw1 = gamma*vw1 + eta * (x.T@w1_delta + nabla*w1)\n",
    "    vb1 = gamma*vb1 + eta * w1_delta.sum(axis=0)\n",
    "    \n",
    "    \n",
    "    w3 -= vw3\n",
    "    b3 -= vb3\n",
    "\n",
    "    w2 -= vw2\n",
    "    b2 -= vb2\n",
    "\n",
    "    w1 -= vw1\n",
    "    b1 -= vb1\n",
    "    \n",
    "    weights = [w1,w2,w3,b1,b2,b3]\n",
    "    cache = [vw1,vw2,vw3,vb1,vb2,vb3]\n",
    "    \n",
    "    return weights, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(weights, x_train, y_train, x_valid, y_valid, epochs = 10, nbatchs=25, alpha = 1e-3, decay = 0, momentum = 0, l2 = 0.001, dropout_percent = 0):\n",
    "    \n",
    "    pross = x_train.shape[0]*0.05\n",
    "    \n",
    "    history = [[],[]]\n",
    "    \n",
    "    index = np.arange(x_train.shape[0])\n",
    "    cache = None\n",
    "    print(\"Train data: %d\" % (x_train.shape[0]))\n",
    "    print(\"Validation data: %d \\n\" % (x_valid.shape[0]))\n",
    "    mtime = 0\n",
    "    \n",
    "    r_weights = []\n",
    "    max_accuracy_valid = 0\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        np.random.shuffle(index)\n",
    "        t = 0\n",
    "        iterations = round(x_train.shape[0]/nbatchs)\n",
    "        prog = \"\"\n",
    "        sacurr = 0\n",
    "        sloss = 0\n",
    "        sys.stdout.write(\"\\nEpochs: %2d \\ %2d \\n\"% (j+1,epochs))\n",
    "        stime = 0\n",
    "        timeIT = time.time()\n",
    "        for i in range(iterations):\n",
    "            timeI = time.time()\n",
    "            f = i*nbatchs\n",
    "            l = f+nbatchs\n",
    "            \n",
    "            if(l>(x_train.shape[0]-1)):\n",
    "                l = x_train.shape[0]\n",
    "                \n",
    "            x = np.array([elastic_transform(xx.reshape(28,28),15,3).reshape(784) for xx in x_train[index[f:l]]])\n",
    "            y = y_train[index[f:l]]\n",
    "\n",
    "            outputs = predict(weights, x, dropout_percent)\n",
    "            \n",
    "            loss = cost(outputs[-1], y, weights, l2)\n",
    "            \n",
    "            \n",
    "            accuracy_t = accuracy(outputs[-1], y)\n",
    "            \n",
    "            sacurr += accuracy_t\n",
    "            sloss += loss\n",
    "            \n",
    "            accuracy_train = sacurr/(i+1)\n",
    "            loss_train = sloss/(i+1)\n",
    "            \n",
    "            weights, cache = SGD(weights, x, y, outputs, alpha, momentum, l2, cache)\n",
    "            \n",
    "            t+= x.shape[0]\n",
    "            \n",
    "            qtd = round(t/pross)\n",
    "            prog = \"[\"\n",
    "            for p in range(20):\n",
    "                if(p<qtd-1):\n",
    "                    prog += \"=\"\n",
    "                elif(p==qtd-1):\n",
    "                    prog += \">\"\n",
    "                else:\n",
    "                    prog += \".\"\n",
    "            prog += \"]\"\n",
    "\n",
    "            \n",
    "            stime += time.time()-timeI\n",
    "            mtime = stime/(i+1)\n",
    "            mTimeT = mtime * (iterations-i-1)\n",
    "            \n",
    "            sys.stdout.write(\"\\r%5d/%5d %s ETA: %3d s - loss: %.4f  acc: %.4f\" % (t, x_train.shape[0], prog, mTimeT, loss_train, accuracy_train))\n",
    "            \n",
    "            history[0].append([loss_train, accuracy_train])\n",
    "        mtime = time.time()-timeIT\n",
    "        alpha = alpha - (alpha*decay)\n",
    "        \n",
    "        outputs = predict(weights, x_valid)\n",
    "        \n",
    "        loss_valid = cost(outputs[-1], y_valid, weights, l2)\n",
    "        accuracy_valid = accuracy(outputs[-1], y_valid)\n",
    "        \n",
    "        sys.stdout.write(\"\\r%5d/%5d %s ETA: %3d s loss: %.4f  acc: %.4f - lossValid: %.4f  accValid: %.4f \" % ( t, x_train.shape[0], prog, mtime, loss_train, accuracy_train, loss_valid, accuracy_valid))\n",
    "        history[1].append([loss_valid, accuracy_valid])\n",
    "            \n",
    "        if(accuracy_valid>=max_accuracy_valid):\n",
    "            w1,w2,w3,b1,b2,b3  = weights\n",
    "            r_weights = [w1.copy(),w2.copy(),w3.copy(),b1.copy(),b2.copy(),b3.copy()]\n",
    "            max_accuracy_valid = accuracy_valid\n",
    "        \n",
    "    return r_weights, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 60000\n",
      "Validation data: 10000 \n",
      "\n",
      "\n",
      "Epochs:  1 \\ 40 \n",
      "60000/60000 [===================>] ETA:  48 s loss: 0.0809  acc: 84.5933 - lossValid: 0.0190  accValid: 96.6400 \n",
      "Epochs:  2 \\ 40 \n",
      "60000/60000 [===================>] ETA:  48 s loss: 0.0433  acc: 92.1567 - lossValid: 0.0144  accValid: 97.4200 \n",
      "Epochs:  3 \\ 40 \n",
      "60000/60000 [===================>] ETA:  49 s loss: 0.0358  acc: 93.6400 - lossValid: 0.0128  accValid: 97.8200 \n",
      "Epochs:  4 \\ 40 \n",
      "60000/60000 [===================>] ETA:  51 s loss: 0.0313  acc: 94.4233 - lossValid: 0.0101  accValid: 98.2500 \n",
      "Epochs:  5 \\ 40 \n",
      "60000/60000 [===================>] ETA:  52 s loss: 0.0290  acc: 94.8450 - lossValid: 0.0091  accValid: 98.4600 \n",
      "Epochs:  6 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0270  acc: 95.1717 - lossValid: 0.0087  accValid: 98.3800 \n",
      "Epochs:  7 \\ 40 \n",
      "60000/60000 [===================>] ETA:  56 s loss: 0.0248  acc: 95.6450 - lossValid: 0.0079  accValid: 98.6200 \n",
      "Epochs:  8 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0238  acc: 95.6883 - lossValid: 0.0082  accValid: 98.6300 \n",
      "Epochs:  9 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0225  acc: 95.9700 - lossValid: 0.0067  accValid: 98.7800 \n",
      "Epochs: 10 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0220  acc: 96.2833 - lossValid: 0.0066  accValid: 98.8200 \n",
      "Epochs: 11 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0215  acc: 96.2250 - lossValid: 0.0068  accValid: 98.7200 \n",
      "Epochs: 12 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0206  acc: 96.4033 - lossValid: 0.0065  accValid: 98.8800 \n",
      "Epochs: 13 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0202  acc: 96.4317 - lossValid: 0.0061  accValid: 98.8700 \n",
      "Epochs: 14 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0190  acc: 96.6900 - lossValid: 0.0067  accValid: 98.8500 \n",
      "Epochs: 15 \\ 40 \n",
      "60000/60000 [===================>] ETA:  53 s loss: 0.0187  acc: 96.7300 - lossValid: 0.0063  accValid: 98.8500 \n",
      "Epochs: 16 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0186  acc: 96.7150 - lossValid: 0.0057  accValid: 99.0200 \n",
      "Epochs: 17 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0176  acc: 97.0017 - lossValid: 0.0056  accValid: 98.9200 \n",
      "Epochs: 18 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0174  acc: 96.9400 - lossValid: 0.0057  accValid: 99.0600 \n",
      "Epochs: 19 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0175  acc: 96.9250 - lossValid: 0.0051  accValid: 99.0800 \n",
      "Epochs: 20 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0170  acc: 96.9967 - lossValid: 0.0052  accValid: 99.0600 \n",
      "Epochs: 21 \\ 40 \n",
      "60000/60000 [===================>] ETA:  56 s loss: 0.0166  acc: 97.0833 - lossValid: 0.0054  accValid: 99.1500 \n",
      "Epochs: 22 \\ 40 \n",
      "60000/60000 [===================>] ETA:  56 s loss: 0.0162  acc: 97.1550 - lossValid: 0.0050  accValid: 99.1000 \n",
      "Epochs: 23 \\ 40 \n",
      "60000/60000 [===================>] ETA:  55 s loss: 0.0162  acc: 97.1083 - lossValid: 0.0051  accValid: 99.0800 \n",
      "Epochs: 24 \\ 40 \n",
      "60000/60000 [===================>] ETA:  56 s loss: 0.0158  acc: 97.2450 - lossValid: 0.0052  accValid: 99.1400 \n",
      "Epochs: 25 \\ 40 \n",
      "60000/60000 [===================>] ETA:  53 s loss: 0.0155  acc: 97.3250 - lossValid: 0.0052  accValid: 99.0800 \n",
      "Epochs: 26 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0151  acc: 97.3250 - lossValid: 0.0050  accValid: 99.0300 \n",
      "Epochs: 27 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0149  acc: 97.3317 - lossValid: 0.0048  accValid: 99.1100 \n",
      "Epochs: 28 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0149  acc: 97.4483 - lossValid: 0.0048  accValid: 99.1500 \n",
      "Epochs: 29 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0151  acc: 97.3817 - lossValid: 0.0044  accValid: 99.1500 \n",
      "Epochs: 30 \\ 40 \n",
      "60000/60000 [===================>] ETA:  53 s loss: 0.0145  acc: 97.5000 - lossValid: 0.0048  accValid: 99.0900 \n",
      "Epochs: 31 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0145  acc: 97.4650 - lossValid: 0.0045  accValid: 99.1600 \n",
      "Epochs: 32 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0146  acc: 97.4550 - lossValid: 0.0045  accValid: 99.1600 \n",
      "Epochs: 33 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0146  acc: 97.4583 - lossValid: 0.0044  accValid: 99.1800 \n",
      "Epochs: 34 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0140  acc: 97.5700 - lossValid: 0.0047  accValid: 99.1500 \n",
      "Epochs: 35 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0140  acc: 97.5283 - lossValid: 0.0043  accValid: 99.1500 \n",
      "Epochs: 36 \\ 40 \n",
      "60000/60000 [===================>] ETA:  53 s loss: 0.0137  acc: 97.6117 - lossValid: 0.0042  accValid: 99.2000 \n",
      "Epochs: 37 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0139  acc: 97.5883 - lossValid: 0.0041  accValid: 99.2800 \n",
      "Epochs: 38 \\ 40 \n",
      "60000/60000 [===================>] ETA:  54 s loss: 0.0136  acc: 97.6200 - lossValid: 0.0042  accValid: 99.2500 \n",
      "Epochs: 39 \\ 40 \n",
      "60000/60000 [===================>] ETA:  48 s loss: 0.0133  acc: 97.7317 - lossValid: 0.0042  accValid: 99.1900 \n",
      "Epochs: 40 \\ 40 \n",
      "60000/60000 [===================>] ETA:  48 s loss: 0.0135  acc: 97.6933 - lossValid: 0.0042  accValid: 99.1900 "
     ]
    }
   ],
   "source": [
    "weights = CreateWeights()\n",
    "\n",
    "alpha = 5e-2\n",
    "epochs = 10\n",
    "nbatchs = 100\n",
    "weights, history = run(weights, \n",
    "              X_train, Y_train, \n",
    "              X_test, Y_test, \n",
    "              epochs = epochs,\n",
    "              nbatchs=nbatchs, \n",
    "              alpha = alpha, \n",
    "              decay = 0.05, \n",
    "              momentum = 0.9, \n",
    "              l2 = 1e-3, \n",
    "              dropout_percent = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
