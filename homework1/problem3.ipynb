{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from keras.datasets import mnist \n",
    "from keras.utils import np_utils\n",
    "\n",
    "# the data, shuffled and split between train and test sets \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "input_dim = 784 #28*28 \n",
    "X_train = X_train.reshape(60000, input_dim) \n",
    "X_test = X_test.reshape(10000, input_dim) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255\n",
    "nb_classes = 10 \n",
    "m=X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_exp = np.exp(z - np.max(z, -1, keepdims=True))\n",
    "    return (z_exp / np.sum(z_exp, -1, keepdims=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cce(a, y):\n",
    "    return -np.sum(y * np.log(a), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,x):\n",
    "        return softmax(x.dot(w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_predic,y_origin):\n",
    "    temp=np.zeros_like(y_origin)\n",
    "    for i in range(y_origin.shape[0]):\n",
    "        if np.argmax(y_predic[i])==y_origin[i]:\n",
    "            temp[i]=0\n",
    "        else:\n",
    "            temp[i]=1\n",
    "        \n",
    "    return (100.0 - np.mean(temp*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    epochs=100\n",
    "    n_class=10\n",
    "    lr=0.01\n",
    "    batch_size=4\n",
    "    weights = np.zeros((X_train.shape[1], n_class))\n",
    "    bias = np.random.randn(1, n_class)\n",
    "    history = {\n",
    "            'loss': []\n",
    "    }\n",
    "    weights_history = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1,epochs))\n",
    "        \n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_train_shuffled = X_train[shuffled_indices]\n",
    "        Y_train_shuffled = Y_train[shuffled_indices]\n",
    "        loss = []\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            xi = X_train_shuffled[i:i+batch_size]\n",
    "            yi = Y_train_shuffled[i:i+batch_size]\n",
    "            z = xi.dot(weights) + bias\n",
    "            a = softmax(z)\n",
    "\n",
    "            gradient = a - yi\n",
    "                \n",
    "            weights = weights - lr * (gradient.T @ xi).T/batch_size\n",
    "            bias = bias - lr * gradient.mean(axis=0)\n",
    "                \n",
    "            if i % (X_train.shape[0]/30) == 0:\n",
    "                weights_history += [softmax(weights)]\n",
    "                \n",
    "        history['loss'] += [cce(predict(weights, bias, X_train_shuffled), Y_train_shuffled).mean()]\n",
    "        print(\"Loss= {}\\n\".format(history['loss'][-1]))\n",
    "                \n",
    "    pred_train = predict(weights, bias, X_train)\n",
    "    print(\"Accuracy Train: \",get_acc(pred_train,y_train))\n",
    "    pred_test = predict(weights, bias,X_test)\n",
    "    print(\"Accuarcy Test:  \",get_acc(pred_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Loss= 0.33469641581631737\n",
      "\n",
      "Epoch 2/100\n",
      "Loss= 0.3026566000810552\n",
      "\n",
      "Epoch 3/100\n",
      "Loss= 0.29065051110726187\n",
      "\n",
      "Epoch 4/100\n",
      "Loss= 0.28199820262472464\n",
      "\n",
      "Epoch 5/100\n",
      "Loss= 0.2804839954764624\n",
      "\n",
      "Epoch 6/100\n",
      "Loss= 0.27331166766207576\n",
      "\n",
      "Epoch 7/100\n",
      "Loss= 0.2695757272348435\n",
      "\n",
      "Epoch 8/100\n",
      "Loss= 0.26816170285301055\n",
      "\n",
      "Epoch 9/100\n",
      "Loss= 0.26558396775467963\n",
      "\n",
      "Epoch 10/100\n",
      "Loss= 0.2642129736351226\n",
      "\n",
      "Epoch 11/100\n",
      "Loss= 0.26155659642147194\n",
      "\n",
      "Epoch 12/100\n",
      "Loss= 0.25877276673306887\n",
      "\n",
      "Epoch 13/100\n",
      "Loss= 0.25868075635791526\n",
      "\n",
      "Epoch 14/100\n",
      "Loss= 0.2603023905456453\n",
      "\n",
      "Epoch 15/100\n",
      "Loss= 0.25467450981265516\n",
      "\n",
      "Epoch 16/100\n",
      "Loss= 0.25798103567752523\n",
      "\n",
      "Epoch 17/100\n",
      "Loss= 0.25641988132970345\n",
      "\n",
      "Epoch 18/100\n",
      "Loss= 0.2521800716054014\n",
      "\n",
      "Epoch 19/100\n",
      "Loss= 0.25147873119678543\n",
      "\n",
      "Epoch 20/100\n",
      "Loss= 0.2522317748352751\n",
      "\n",
      "Epoch 21/100\n",
      "Loss= 0.25127037880805836\n",
      "\n",
      "Epoch 22/100\n",
      "Loss= 0.25074281509737256\n",
      "\n",
      "Epoch 23/100\n",
      "Loss= 0.24865767105828337\n",
      "\n",
      "Epoch 24/100\n",
      "Loss= 0.2499747234650532\n",
      "\n",
      "Epoch 25/100\n",
      "Loss= 0.24915614687315635\n",
      "\n",
      "Epoch 26/100\n",
      "Loss= 0.2522086859004945\n",
      "\n",
      "Epoch 27/100\n",
      "Loss= 0.2487394006997875\n",
      "\n",
      "Epoch 28/100\n",
      "Loss= 0.246407071871598\n",
      "\n",
      "Epoch 29/100\n",
      "Loss= 0.24620613513919803\n",
      "\n",
      "Epoch 30/100\n",
      "Loss= 0.2462281999201564\n",
      "\n",
      "Epoch 31/100\n",
      "Loss= 0.2475697959740824\n",
      "\n",
      "Epoch 32/100\n",
      "Loss= 0.24499793444967222\n",
      "\n",
      "Epoch 33/100\n",
      "Loss= 0.2456199447566653\n",
      "\n",
      "Epoch 34/100\n",
      "Loss= 0.24373004025947453\n",
      "\n",
      "Epoch 35/100\n",
      "Loss= 0.24355383749633092\n",
      "\n",
      "Epoch 36/100\n",
      "Loss= 0.2442046468803745\n",
      "\n",
      "Epoch 37/100\n",
      "Loss= 0.2431711333358176\n",
      "\n",
      "Epoch 38/100\n",
      "Loss= 0.24375323842633315\n",
      "\n",
      "Epoch 39/100\n",
      "Loss= 0.2413714024753034\n",
      "\n",
      "Epoch 40/100\n",
      "Loss= 0.24154574856729794\n",
      "\n",
      "Epoch 41/100\n",
      "Loss= 0.24062232568921987\n",
      "\n",
      "Epoch 42/100\n",
      "Loss= 0.24152888333929565\n",
      "\n",
      "Epoch 43/100\n",
      "Loss= 0.24373803078316522\n",
      "\n",
      "Epoch 44/100\n",
      "Loss= 0.24320110925612215\n",
      "\n",
      "Epoch 45/100\n",
      "Loss= 0.24287363127177028\n",
      "\n",
      "Epoch 46/100\n",
      "Loss= 0.23980646290032406\n",
      "\n",
      "Epoch 47/100\n",
      "Loss= 0.24062661911757527\n",
      "\n",
      "Epoch 48/100\n",
      "Loss= 0.2412867904766221\n",
      "\n",
      "Epoch 49/100\n",
      "Loss= 0.23940025096841275\n",
      "\n",
      "Epoch 50/100\n",
      "Loss= 0.24018413911382397\n",
      "\n",
      "Epoch 51/100\n",
      "Loss= 0.2396139965539582\n",
      "\n",
      "Epoch 52/100\n",
      "Loss= 0.24068353571486467\n",
      "\n",
      "Epoch 53/100\n",
      "Loss= 0.23821908056836516\n",
      "\n",
      "Epoch 54/100\n",
      "Loss= 0.2394704634658908\n",
      "\n",
      "Epoch 55/100\n",
      "Loss= 0.23777619311265402\n",
      "\n",
      "Epoch 56/100\n",
      "Loss= 0.23872281380085078\n",
      "\n",
      "Epoch 57/100\n",
      "Loss= 0.23803025999661467\n",
      "\n",
      "Epoch 58/100\n",
      "Loss= 0.2364869719322851\n",
      "\n",
      "Epoch 59/100\n",
      "Loss= 0.23727657138920671\n",
      "\n",
      "Epoch 60/100\n",
      "Loss= 0.2364196324682025\n",
      "\n",
      "Epoch 61/100\n",
      "Loss= 0.23982306105739168\n",
      "\n",
      "Epoch 62/100\n",
      "Loss= 0.23639338306598365\n",
      "\n",
      "Epoch 63/100\n",
      "Loss= 0.2383533408121414\n",
      "\n",
      "Epoch 64/100\n",
      "Loss= 0.23816484209562014\n",
      "\n",
      "Epoch 65/100\n",
      "Loss= 0.24037609939267365\n",
      "\n",
      "Epoch 66/100\n",
      "Loss= 0.23551537101046136\n",
      "\n",
      "Epoch 67/100\n",
      "Loss= 0.23544449067503104\n",
      "\n",
      "Epoch 68/100\n",
      "Loss= 0.23734608648588001\n",
      "\n",
      "Epoch 69/100\n",
      "Loss= 0.23520943463221583\n",
      "\n",
      "Epoch 70/100\n",
      "Loss= 0.23607358349084376\n",
      "\n",
      "Epoch 71/100\n",
      "Loss= 0.235944993977302\n",
      "\n",
      "Epoch 72/100\n",
      "Loss= 0.23436643013286101\n",
      "\n",
      "Epoch 73/100\n",
      "Loss= 0.23691543317861222\n",
      "\n",
      "Epoch 74/100\n",
      "Loss= 0.23543705011007768\n",
      "\n",
      "Epoch 75/100\n",
      "Loss= 0.23512989260733558\n",
      "\n",
      "Epoch 76/100\n",
      "Loss= 0.23424884320794\n",
      "\n",
      "Epoch 77/100\n",
      "Loss= 0.23591352656155495\n",
      "\n",
      "Epoch 78/100\n",
      "Loss= 0.23559397992338207\n",
      "\n",
      "Epoch 79/100\n",
      "Loss= 0.23448796970580452\n",
      "\n",
      "Epoch 80/100\n",
      "Loss= 0.2345795263936262\n",
      "\n",
      "Epoch 81/100\n",
      "Loss= 0.23516259198117423\n",
      "\n",
      "Epoch 82/100\n",
      "Loss= 0.23442880006084613\n",
      "\n",
      "Epoch 83/100\n",
      "Loss= 0.2433214796626901\n",
      "\n",
      "Epoch 84/100\n",
      "Loss= 0.23480604244707695\n",
      "\n",
      "Epoch 85/100\n",
      "Loss= 0.23251803636697893\n",
      "\n",
      "Epoch 86/100\n",
      "Loss= 0.233878435982448\n",
      "\n",
      "Epoch 87/100\n",
      "Loss= 0.23213513886274023\n",
      "\n",
      "Epoch 88/100\n",
      "Loss= 0.2330311357105902\n",
      "\n",
      "Epoch 89/100\n",
      "Loss= 0.23497858864140905\n",
      "\n",
      "Epoch 90/100\n",
      "Loss= 0.23276866400277463\n",
      "\n",
      "Epoch 91/100\n",
      "Loss= 0.2344130552301094\n",
      "\n",
      "Epoch 92/100\n",
      "Loss= 0.23326377200660797\n",
      "\n",
      "Epoch 93/100\n",
      "Loss= 0.2334862204709701\n",
      "\n",
      "Epoch 94/100\n",
      "Loss= 0.23477999865838237\n",
      "\n",
      "Epoch 95/100\n",
      "Loss= 0.23463769871158646\n",
      "\n",
      "Epoch 96/100\n",
      "Loss= 0.23258040750146983\n",
      "\n",
      "Epoch 97/100\n",
      "Loss= 0.23217035400483033\n",
      "\n",
      "Epoch 98/100\n",
      "Loss= 0.23335273971308174\n",
      "\n",
      "Epoch 99/100\n",
      "Loss= 0.2325878150356929\n",
      "\n",
      "Epoch 100/100\n",
      "Loss= 0.2305402965756227\n",
      "\n",
      "Accuracy Train:  93.68333333333334\n",
      "Accuarcy Test:   92.6\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
